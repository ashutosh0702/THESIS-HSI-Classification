{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSI Classification: Local Similarity Analysis\n",
    "\n",
    "This notebook demonstrates the hyperspectral image classification pipeline with focus on **local similarity metrics** for thesis research.\n",
    "\n",
    "## Contents\n",
    "1. Data Loading & Preprocessing\n",
    "2. Patch Extraction\n",
    "3. Similarity Metrics Exploration\n",
    "4. Model Training with MLflow\n",
    "5. Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our modules\n",
    "from src.data import load_hsi, load_benchmark_dataset, preprocess_pipeline\n",
    "from src.features import extract_patches, split_dataset\n",
    "from src.models import (\n",
    "    rbf_similarity, sam_similarity, local_similarity_matrix,\n",
    "    SVMClassifier, create_model, train_svm, train_neural_network,\n",
    "    compute_all_metrics, print_classification_report\n",
    ")\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri('mlruns')\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Data (for testing)\n",
    "\n",
    "Replace this with actual HSI data loading in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic hyperspectral cube for demonstration\n",
    "np.random.seed(42)\n",
    "\n",
    "# Dimensions\n",
    "height, width, n_bands = 100, 100, 200\n",
    "n_classes = 5\n",
    "\n",
    "# Create class-specific spectral signatures\n",
    "class_spectra = np.random.randn(n_classes, n_bands)\n",
    "\n",
    "# Generate ground truth\n",
    "ground_truth = np.zeros((height, width), dtype=np.int32)\n",
    "for c in range(n_classes):\n",
    "    mask = np.random.random((height, width)) < (1 / n_classes)\n",
    "    ground_truth[mask] = c\n",
    "\n",
    "# Generate HSI cube based on ground truth\n",
    "hsi_cube = np.zeros((height, width, n_bands), dtype=np.float32)\n",
    "for c in range(n_classes):\n",
    "    mask = ground_truth == c\n",
    "    hsi_cube[mask] = class_spectra[c] + 0.1 * np.random.randn(np.sum(mask), n_bands)\n",
    "\n",
    "print(f\"HSI Cube shape: {hsi_cube.shape}\")\n",
    "print(f\"Ground Truth shape: {ground_truth.shape}\")\n",
    "print(f\"Classes: {np.unique(ground_truth)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing pipeline\n",
    "result = preprocess_pipeline(\n",
    "    hsi_cube,\n",
    "    remove_water=False,  # No water bands in synthetic data\n",
    "    reduce_dims='pca',\n",
    "    n_components=30,\n",
    "    normalize='minmax',\n",
    "    log_to_mlflow=False\n",
    ")\n",
    "\n",
    "processed_data = result['data']\n",
    "print(f\"Processed shape: {processed_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Patch Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patches\n",
    "window_size = 5\n",
    "dataset = extract_patches(\n",
    "    processed_data,\n",
    "    ground_truth,\n",
    "    window_size=window_size,\n",
    "    include_background=False,\n",
    "    log_to_mlflow=False\n",
    ")\n",
    "\n",
    "print(f\"Patches shape: {dataset.patches.shape}\")\n",
    "print(f\"Number of samples: {dataset.n_samples}\")\n",
    "\n",
    "# Split into train/val/test\n",
    "train_set, val_set, test_set = split_dataset(\n",
    "    dataset,\n",
    "    train_ratio=0.6,\n",
    "    val_ratio=0.2,\n",
    "    test_ratio=0.2,\n",
    "    stratify=True,\n",
    "    log_to_mlflow=False\n",
    ")\n",
    "\n",
    "print(f\"Train: {train_set.n_samples}, Val: {val_set.n_samples}, Test: {test_set.n_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Local Similarity Analysis\n",
    "\n",
    "This is the core of the thesis research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze local similarity for a sample patch\n",
    "sample_patch = train_set.patches[0]\n",
    "\n",
    "# Different similarity metrics\n",
    "sigmas = [0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, len(sigmas), figsize=(16, 8))\n",
    "\n",
    "# RBF similarity with different sigma values\n",
    "for i, sigma in enumerate(sigmas):\n",
    "    sim_matrix = local_similarity_matrix(sample_patch, metric='rbf', sigma=sigma)\n",
    "    \n",
    "    axes[0, i].imshow(sim_matrix, cmap='hot', vmin=0, vmax=1)\n",
    "    axes[0, i].set_title(f'RBF (σ={sigma})')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# SAM vs Euclidean\n",
    "sim_sam = local_similarity_matrix(sample_patch, metric='sam')\n",
    "sim_euc = local_similarity_matrix(sample_patch, metric='euclidean')\n",
    "\n",
    "axes[1, 0].imshow(sim_sam, cmap='hot', vmin=0, vmax=1)\n",
    "axes[1, 0].set_title('SAM Similarity')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(sim_euc, cmap='hot', vmin=0, vmax=1)\n",
    "axes[1, 1].set_title('Euclidean Similarity')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(2, len(sigmas)):\n",
    "    axes[1, j].axis('off')\n",
    "\n",
    "plt.suptitle('Local Similarity Analysis', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare flattened features for SVM\n",
    "X_train = train_set.get_center_pixels()\n",
    "y_train = train_set.labels\n",
    "\n",
    "X_val = val_set.get_center_pixels()\n",
    "y_val = val_set.labels\n",
    "\n",
    "X_test = test_set.get_center_pixels()\n",
    "y_test = test_set.labels\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "\n",
    "# Train SVM with RBF kernel (relates to RBF similarity in thesis)\n",
    "svm = train_svm(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    kernel='rbf',\n",
    "    C=10.0,\n",
    "    gamma='scale',\n",
    "    experiment_name='hsi_classification',\n",
    "    run_name='SVM_RBF_baseline',\n",
    "    log_to_mlflow=True\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = svm.predict(X_test)\n",
    "print_classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Neural Network Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.models import create_model, train_neural_network, create_data_loaders, TrainingConfig\n",
    "\n",
    "# Get dimensions\n",
    "n_bands = train_set.patches.shape[-1]\n",
    "n_classes_nn = len(np.unique(train_set.labels))\n",
    "\n",
    "# Create 3D-CNN model\n",
    "model = create_model(\n",
    "    model_type='3d_cnn',\n",
    "    n_bands=n_bands,\n",
    "    n_classes=n_classes_nn,\n",
    "    patch_size=window_size\n",
    ")\n",
    "print(model)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = create_data_loaders(train_set.patches, train_set.labels, batch_size=32, data_format='3d')\n",
    "val_loader = create_data_loaders(val_set.patches, val_set.labels, batch_size=32, shuffle=False, data_format='3d')\n",
    "\n",
    "# Training config\n",
    "config = TrainingConfig(\n",
    "    experiment_name='hsi_classification',\n",
    "    run_name='3D_CNN_experiment',\n",
    "    n_epochs=20,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "# Train (set log_to_mlflow=True to track with MLflow)\n",
    "model, history = train_neural_network(\n",
    "    model, train_loader, val_loader,\n",
    "    config=config,\n",
    "    similarity_config={'metric': 'patch_based', 'window_size': window_size},\n",
    "    log_to_mlflow=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history['train_acc'], label='Train')\n",
    "axes[1].plot(history['val_acc'], label='Validation')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. MLflow UI\n",
    "\n",
    "Run the following command in terminal to view experiments:\n",
    "\n",
    "```bash\n",
    "cd hsi_classification\n",
    "mlflow ui --port 5000\n",
    "```\n",
    "\n",
    "Then open http://localhost:5000 in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Streamlit Dashboard\n",
    "\n",
    "```bash\n",
    "cd hsi_classification\n",
    "streamlit run src/visualization/visualize.py --server.port 8501\n",
    "```\n",
    "\n",
    "Then open http://localhost:8501 in your browser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
